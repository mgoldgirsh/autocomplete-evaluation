{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths: \n",
      "train 5121 \n",
      "validation 2169 \n",
      "test 2227\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets \n",
    "# Taken From: \n",
    "# https://sites.google.com/site/daehpark/Resources/data-set-for-query-auto-completion-sigir-2017 \n",
    "\n",
    "# @inproceedings{park2017neural,   \n",
    "#                title={A neural language model for query auto-completion},\n",
    "#                author={Park, Dae Hoon and Chiba, Rikio},\n",
    "#                booktitle={Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},\n",
    "#                pages={1189--1192},\n",
    "#                year={2017},\n",
    "#                organization={ACM} }\n",
    "\n",
    "def load_dataset(f: str, sep: str='\\t') -> dict: \n",
    "    \"\"\"Loads the dataset from a file into memory.\n",
    "\n",
    "    Args:\n",
    "        f (str): the file to load the dataset from\n",
    "        sep (str, optional): the seperator to use to delimit the file. Defaults to '\\t'.\n",
    "\n",
    "    Returns:\n",
    "        dict: a dictionary of the QAC data loaded into the memory.\n",
    "    \"\"\"\n",
    "    data = { }  # dictionary with final query : prefixes\n",
    "    with open(f, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            item = line.split(sep)\n",
    "            partial, full = item[0].strip(), item[1].strip()\n",
    "            if full in data: \n",
    "                data[full].append(partial)\n",
    "            else:\n",
    "                data[full] = [partial]\n",
    "    return data\n",
    "\n",
    "qac_train = load_dataset('qac_data/qac_training.tsv')\n",
    "qac_val = load_dataset('qac_data/qac_validation.tsv')\n",
    "qac_test = load_dataset('qac_data/qac_test.tsv')\n",
    "\n",
    "print('Lengths:','\\ntrain',len(qac_train), '\\nvalidation',len(qac_val), '\\ntest', len(qac_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('unique marine', {'unique': 7, 'm': 1, 'ma': 1, 'mar': 1, 'mari': 1, 'marin': 1, 'marine': 1}), ('hampton hotel york pa', {'hampton': 14, 'h': 1, 'ho': 1, 'hot': 1, 'hote': 1, 'hotel': 9, 'y': 1, 'yo': 1, 'yor': 1, 'york': 4, 'p': 1, 'pa': 1}), ('laura banks', {'laura': 6, 'b': 1, 'ba': 1, 'ban': 1, 'bank': 1, 'banks': 1}), ('lightspeed girls barey 19', {'lightspeed': 15, 'g': 1, 'gi': 1, 'gir': 1, 'girl': 1, 'girls': 10, 'b': 1, 'ba': 1, 'bar': 1, 'bare': 1, 'barey': 4, '1': 1, '19': 1}), ('golden nugget las vegas', {'golden': 17, 'n': 1, 'nu': 1, 'nug': 1, 'nugg': 1, 'nugge': 1, 'nugget': 11, 'l': 1, 'la': 1, 'las': 7, 'v': 1, 've': 1, 'veg': 1, 'vega': 1, 'vegas': 1})]\n"
     ]
    }
   ],
   "source": [
    "# Use bag of words to retrieve queries \n",
    "# For the bag of words model all the full queries are our corpus \n",
    "# Therefore we need to compute the similarity of words between the words in the partial query and the full corpus to determine \n",
    "# what possible queries to generate. \n",
    "#   - try without removing stopwords\n",
    "\n",
    "def generate_bag_of_words(dataset: dict) -> dict: \n",
    "    \"\"\"Generates a bag of words per each query, using the partial queries to generate the bag of words. \n",
    "       The bag of words per query will emphasize the key words required for that query based off the partial queries \n",
    "       that should obtain the same results as the original query. \n",
    "    \n",
    "    Args:\n",
    "        dataset (dict): the dataset to generate a bag of words for.\n",
    "\n",
    "    Returns:\n",
    "        dict: the dictionary of queries with their corresponding bag of words.\n",
    "    \"\"\"\n",
    "    bag = {} \n",
    "    for query, partials in dataset.items(): \n",
    "        for partial in partials:\n",
    "            tokens = partial.split(\" \")\n",
    "            if query not in bag:\n",
    "                bag[query] = {} \n",
    "            for token in tokens: \n",
    "                if token in bag[query]:\n",
    "                    bag[query][token] += 1\n",
    "                else: \n",
    "                    bag[query][token] = 1\n",
    "                \n",
    "    return bag\n",
    "\n",
    "bag_of_words = generate_bag_of_words(qac_train)\n",
    "print(list(bag_of_words.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hotel jobs', np.float64(3.9960704593355154)),\n",
       " ('hotel descondido', np.float64(3.7677235759449146)),\n",
       " ('hotel kranenturm', np.float64(3.7677235759449146)),\n",
       " ('hotel l europe amsterdam', np.float64(3.3754226184472573)),\n",
       " ('h l h o h s d j', np.float64(3.1822798397639285))]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now given the full queroes bag of words we now predict the partial query\n",
    "def predict(partial_q: str, bag_of_words: dict, context:str=None, k:int=5) -> list:\n",
    "    \"\"\"Predicts what query the partial question provided should belong to\n",
    "\n",
    "    Args:\n",
    "        partial_q (str): the partial query inputted\n",
    "        bag_of_words (dict): the bag of words associated with the partial query\n",
    "        context (str, optional): the context provided before the partial query. Defaults to None.\n",
    "        k (int, optional): the number of best results to return. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        list : of the k number of best querries to autocomplete to with their associated tf-idf scores.\n",
    "    \"\"\"\n",
    "    probabilities = {}\n",
    "    \n",
    "    # the tokens are a list of the full words in the partial query as well \n",
    "    # as the characters in the partial query, as the bag of words was generated with \n",
    "    # partial queries. \n",
    "    tokens = []\n",
    "    if context is not None:\n",
    "        tokens += context.strip().split(\" \")\n",
    "    tokens += partial_q.strip().split(' ') + list(partial_q)\n",
    "    for query, bag in bag_of_words.items(): \n",
    "        probabilities[query] = 0\n",
    "        tokens_in_query = sum(bag.values())\n",
    "        # Uses tf-idf to calculate the probabilities.\n",
    "        for token in tokens: \n",
    "            if token in bag:\n",
    "                tf = bag[token] / tokens_in_query\n",
    "            else:\n",
    "                tf = 0\n",
    "            idf = np.log2(len(bag_of_words) / len(list(filter(lambda bag: token in bag, bag_of_words.values()))))\n",
    "            probabilities[query] += tf * idf\n",
    "    \n",
    "    # returns the probabilities in descending order (highest first)\n",
    "    sorted_probs = sorted(probabilities.items(), key=lambda item: item[1], reverse=True)\n",
    "    if k is not None: \n",
    "        result = sorted_probs[:k]\n",
    "    else:\n",
    "        result = sorted_probs\n",
    "    return result\n",
    "\n",
    "predict(\"hotel\", bag_of_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
