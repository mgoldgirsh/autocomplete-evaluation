{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths: \n",
      "train 5121 \n",
      "validation 2169 \n",
      "test 2227\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets \n",
    "# Taken From: \n",
    "# https://sites.google.com/site/daehpark/Resources/data-set-for-query-auto-completion-sigir-2017 \n",
    "\n",
    "# @inproceedings{park2017neural,   \n",
    "#                title={A neural language model for query auto-completion},\n",
    "#                author={Park, Dae Hoon and Chiba, Rikio},\n",
    "#                booktitle={Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},\n",
    "#                pages={1189--1192},\n",
    "#                year={2017},\n",
    "#                organization={ACM} }\n",
    "\n",
    "def load_dataset(f: str, sep: str='\\t') -> dict: \n",
    "    \"\"\"Loads the dataset from a file into memory.\n",
    "\n",
    "    Args:\n",
    "        f (str): the file to load the dataset from\n",
    "        sep (str, optional): the seperator to use to delimit the file. Defaults to '\\t'.\n",
    "\n",
    "    Returns:\n",
    "        dict: a dictionary of the QAC data loaded into the memory.\n",
    "    \"\"\"\n",
    "    data = { }  # dictionary with final query : prefixes\n",
    "    with open(f, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            item = line.split(sep)\n",
    "            partial, full = item[0].strip(), item[1].strip()\n",
    "            if full in data: \n",
    "                data[full].append(partial)\n",
    "            else:\n",
    "                data[full] = [partial]\n",
    "    return data\n",
    "\n",
    "qac_train = load_dataset('qac_data/qac_training.tsv')\n",
    "qac_val = load_dataset('qac_data/qac_validation.tsv')\n",
    "qac_test = load_dataset('qac_data/qac_test.tsv')\n",
    "\n",
    "print('Lengths:','\\ntrain',len(qac_train), '\\nvalidation',len(qac_val), '\\ntest', len(qac_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5121, 21371)\n",
      "(5121, 1)\n"
     ]
    }
   ],
   "source": [
    "# Create X,y to use for training the LGBM model\n",
    "# Load the prefixes into X, and the results into y \n",
    "X = []\n",
    "y = []\n",
    "for key, prefixes in qac_train.items():\n",
    "    to_add = set()\n",
    "    for prefix in prefixes:\n",
    "        to_add = to_add.union(set(prefix.split(' ')))\n",
    "    X.append(list(to_add))\n",
    "    y += [key]\n",
    "    \n",
    "# Generate the corpus to use \n",
    "corpus = {}\n",
    "i = 0\n",
    "for row in X: \n",
    "    for element in row:\n",
    "        if element not in corpus:\n",
    "            corpus[element] = i\n",
    "            i+=1\n",
    "\n",
    "# Encode X as a one hot vector\n",
    "def encode_X(phi: list, corpus: dict) -> list:\n",
    "    one_hot_X = []\n",
    "    for row in phi: \n",
    "        one_hot = [0] * len(corpus)\n",
    "        for element in row: \n",
    "            if element in corpus:\n",
    "                one_hot[corpus[element]] = 1\n",
    "        one_hot_X.append(one_hot)\n",
    "    \n",
    "    return np.array(one_hot_X)\n",
    "\n",
    "one_hot_X = encode_X(X, corpus)\n",
    "print(one_hot_X.shape)\n",
    "\n",
    "# Enocde Y as a column vector\n",
    "y = np.reshape(np.array(y), (-1, 1))\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hampton hotel york pa', 1),\n",
       " ('hotel auxiliary aid dwarf', 1),\n",
       " ('lord nelson hotel halifax', 1),\n",
       " ('ritz hotel', 1),\n",
       " ('loew s hotels', 1),\n",
       " ('marriott hotels', 1),\n",
       " ('altanyic city nj hotels', 1),\n",
       " ('hotel 71 chicago il', 1),\n",
       " ('las vegas scheduled boxing dates orleans casino hotel', 1),\n",
       " ('cheap hotels los angeles', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now to predict just create a one hot vector of the elements\n",
    "def predict(partial_q: str, corpus: dict, one_hot_X: np.array, y: np.array, context:str=None, k:int=5) -> list:\n",
    "    tokens = []\n",
    "    if context is not None:\n",
    "        tokens += context.strip().split(' ')\n",
    "    tokens += partial_q.strip().split(' ')\n",
    "    \n",
    "    one_hot_partial = encode_X([tokens], corpus)[0]\n",
    "    # Now that the one hot partial vector is encoded check to see which vector best matches \n",
    "    results = []\n",
    "    for row, label in zip(one_hot_X, y):\n",
    "        results.append((label[0], np.dot(one_hot_partial, row)))\n",
    "    \n",
    "    # Now sort the results and obtain the best\n",
    "    if k is None:\n",
    "        return sorted(results, key=lambda item: item[1], reverse=True)\n",
    "    else: \n",
    "        return sorted(results, key=lambda item: item[1], reverse=True)[:k]\n",
    "    \n",
    "\n",
    "predict(\"hotel\", corpus, one_hot_X, y, k=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
